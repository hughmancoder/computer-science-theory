{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data\n",
    "Note: some of this code was adapted from a friends assignment as I liked his code style and found his implementation to be extremely efficient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"mnist.csv\", header=None)\n",
    "features = data.values[:,1:]\n",
    "labels = data.values[:,[0]]\n",
    "print(\"Labels:\\n\",labels,\"\\nFeatures:\\n\",features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images before pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [[]]*6000\n",
    "for i in range(100):\n",
    "    images[i] = features[i].reshape(28,28)\n",
    "    # plt.imshow(images[i], interpolation='nearest')\n",
    "    # plt.show()\n",
    "_, axs = plt.subplots(10, 10, figsize=(28, 28))\n",
    "axs = axs.flatten()\n",
    "incremeter = 0\n",
    "for img, ax in zip(images, axs):\n",
    "    ax.imshow(img)\n",
    "    ax.set_title('Image of number: {}'.format(labels[incremeter]))\n",
    "    incremeter = incremeter + 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Perform PCA on the dataset to reduce each sample into a 10-dimensional feature vector. Show the covariance matrix of the transformed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(features,dimensions):    \n",
    "    # Importing feature vector\n",
    "    X = features\n",
    "    n,d = X.shape\n",
    "\n",
    "    # Centralising Data\n",
    "    M = np.mean(X.T, axis=1)\n",
    "    X_c = X - M\n",
    "\n",
    "    # Calculating the covariance matrix\n",
    "    Cov = 1/n * np.matmul(X_c.T,X_c)\n",
    "\n",
    "    # Calculating Eigen vectors and values\n",
    "    values, vectors = np.linalg.eigh(Cov)\n",
    "\n",
    "    # Select the last 10 Eigen vectors since ascending order\n",
    "    D = np.diag(values)\n",
    "    P = vectors[:,784-dimensions:784]\n",
    "    print(values.shape)\n",
    "    print(D.shape)\n",
    "\n",
    "    # Calculated final X\n",
    "    X_new = np.dot(X_c, P)\n",
    "    Cov_transformed = 1/n * np.matmul(X_new.T,X_new)\n",
    "\n",
    "    # Reconstruct original\n",
    "    Reconstructed = np.matmul(X_new,P.T)\n",
    "    return Reconstructed, Cov_transformed\n",
    "\n",
    "def pca(X, dimensions):\n",
    "    X_c = X - np.mean(X , axis = 0)\n",
    "    cov_mat = np.cov(X_c , rowvar = False)\n",
    "    # for symmetric arrays\n",
    "    eigen_values , eigen_vectors = np.linalg.eigh(cov_mat)\n",
    "    # take the last 10 eigen vectors from columns as the column v[:, i] is the normalized eigenvector corresponding to the\n",
    "    P = eigen_vectors[:,-dimensions:]\n",
    "    # compute the result data \n",
    "    X_compressed = X_c @ P\n",
    "    print(\"dimension of compressed data is \", X_compressed.shape)\n",
    "    # Reconstruct original data by transforming back\n",
    "    reconstructed = X_compressed @ P.T\n",
    "    return reconstructed, np.cov(X_compressed , rowvar = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from examining eigen value drop off, most of the data is contained in the maximumum 12-20 eigen values (dimensions)\n",
    "dimensions = 20\n",
    "reduced_data, transformed_cov = pca(features, dimensions)\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})\n",
    "print(\"Transformed Covariance Matrix:\\n\",transformed_cov)\n",
    "print(\"dimensions of reduced data\", reduced_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconsturcting first 100 images\n",
    "https://stackoverflow.com/questions/41793931/plotting-images-side-by-side-using-matplotlib/54681765#54681765"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [[]]*100\n",
    "for i in range(100):\n",
    "    # convert 784 pixels into a 28 * 28 matrix\n",
    "    images[i] = reduced_data[i].reshape(28,28)\n",
    "\n",
    "_, axs = plt.subplots(10, 10, figsize=(28, 28))\n",
    "axs = axs.flatten()\n",
    "incremeter = 0\n",
    "for img, ax in zip(images, axs):\n",
    "    ax.imshow(img)\n",
    "    ax.set_title('Image of number: {}'.format(labels[incremeter]))\n",
    "    incremeter = incremeter + 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question 2\n",
    "Perform k-means clustering to cluster the dataset (without applying PCA) into 10 groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(features,k,if_plot):\n",
    "    X = features\n",
    "    n,d = X.shape\n",
    "\n",
    "    # 1. Initialise variables and randomly chose inital centers\n",
    "    np.random.seed(1)\n",
    "    centers = X[np.random.choice(n, k, replace=False)]    \n",
    "    closest_centers = np.zeros(n).astype(int)\n",
    "    distances_to_centers = np.zeros((n,k))\n",
    "\n",
    "    # While closest centers is decreasing\n",
    "    iterations = 0\n",
    "    while True:\n",
    "        old_closet_centers = closest_centers.copy()\n",
    "        # 2a. Assign labels based on centers and Find closest centers\n",
    "        for i in range(k):\n",
    "            # taking euclidean norm (distance)\n",
    "            distances_to_centers[:,i] = ((X-centers[i])**2).sum(axis=1)**0.5\n",
    "        closest_centers = np.argmin(distances_to_centers, axis=1)\n",
    "\n",
    "        # 2b. Find new centers/centroids from means of points\n",
    "        for i in range(k):\n",
    "            centers[i,:] = X[closest_centers == i].mean(axis=0)\n",
    "\n",
    "        # Plot loss curve\n",
    "        if (if_plot == 1):\n",
    "            # get min of array\n",
    "            min_distances = np.amin(distances_to_centers, axis=1)\n",
    "            loss = np.sum(min_distances)\n",
    "            plt.scatter(iterations, loss)\n",
    "\n",
    "        # 2c. Check for convergence\n",
    "        if np.all(closest_centers == old_closet_centers):\n",
    "            break\n",
    "        iterations = iterations + 1\n",
    "        \n",
    "    # Setup plot\n",
    "    if (if_plot == 1):\n",
    "        plt.xlabel(\"Interation\")\n",
    "        plt.ylabel(\"Loss (J)\")\n",
    "        plt.title(\"Loss Curve vs Iteration for K=10\")\n",
    "        plt.show()\n",
    "\n",
    "    # Return final\n",
    "    loss = 0\n",
    "    for i in range(n):\n",
    "        loss = loss + ((distances_to_centers[i][closest_centers[i]])**2)**0.5\n",
    "    return closest_centers, centers, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Please plot the loss curve, that is, the change of loss value of the k-means algorithm with respect to the number of iterations. Store loss as we compute k-means in original function.\n",
    "\n",
    "From the plot graph seems to converge around 10-12 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 clusters\n",
    "k=20\n",
    "labels_kmeans, centers_kmeans, loss_kmeans = kmeans(features,k,1)\n",
    "print(\"clustering\", labels_kmeans)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"resultant clustering is\")\n",
    "clusters = []\n",
    "images = [[]]*10\n",
    "for i in range(k):\n",
    "    clusters.append(features[labels_kmeans == i])\n",
    "    for j in range(10):\n",
    "        images[j] = clusters[i][j].reshape(28,28)\n",
    "    print(\"Images in cluster {}:\".format(i))\n",
    "    _, axs = plt.subplots(1, 10, figsize=(28, 28))\n",
    "    axs = axs.flatten()\n",
    "    for img, ax in zip(images, axs):\n",
    "        ax.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "Please use the first 4000 samples as the training set and remaining 2000 samples as the validation set, and design a way to choose the best k in k-means algorithm. Please copy your code snippet here. Get cluster means from training set. Validation set is used to compute loss\n",
    "\n",
    "**Training set:** A set of examples used for learning, that is to fit the parameters of the classifier.\n",
    "\n",
    "**Validation set:** A set of examples used to tune the parameters of a classifier, for example to choose the number of hidden units in a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Dataset into training and validation sets\n",
    "labels_train = labels[:4000]\n",
    "labels_validation = labels[4000:6000]\n",
    "features_train = features[:-2000]\n",
    "features_validaton = features[-2000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find validation set loss\n",
    "def Validation_kmeans(features_validaton, labels_validation, centers_training, k):\n",
    "    n,d = features_validaton.shape\n",
    "\n",
    "    # Assign labels based on centers and Find closest centers\n",
    "    distances_to_centers = np.zeros((n,k))\n",
    "    for i in range(k):\n",
    "        distances_to_centers[:,i] = ((features_validaton-centers_training[i])**2).sum(axis=1)**0.5\n",
    "    closest_centers = np.argmin(distances_to_centers, axis=1)\n",
    "\n",
    "    # Calculate loss\n",
    "    min_distances = np.amin(distances_to_centers, axis=1)\n",
    "    loss = np.sum(min_distances)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick inital range for K\n",
    "K_1 = [2,10,100,500,1000,1500,2000]\n",
    "# K_1 = [10]\n",
    "\n",
    "# For each K train and validation\n",
    "for k in K_1:\n",
    "    print(\"K Means Clustering for K = {}\".format(k))\n",
    "    labels_training, centers_training, loss_training = kmeans(features_train, k, 0)\n",
    "    loss = Validation_kmeans(features_validaton, labels_validation, centers_training, k)\n",
    "    print(\"Loss for K = {} is: {:.0f}\".format(k,loss))\n",
    "    plt.scatter(k, loss)\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"Loss (J)\")\n",
    "plt.title(\"Loss vs K \")\n",
    "plt.show()\n",
    "\n",
    "## Runtime approx 2m 30s ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick 3nd range for K\n",
    "K_3 = [1400,1450,1500,1550,1600]\n",
    "\n",
    "# For each K train and validation\n",
    "for k in K_3:\n",
    "    print(\"K Means Clustering for K = {}\".format(k))\n",
    "    labels_training, centers_training, loss_training = kmeans(features_train, k, 0)\n",
    "    loss = Validation_kmeans(features_validaton, labels_validation, centers_training, k)\n",
    "    print(\"Loss for K = {} is: {:.0f}\".format(k,loss))\n",
    "    plt.scatter(k, loss)\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"Loss (J)\")\n",
    "plt.title(\"Loss vs K \")\n",
    "plt.show()\n",
    "\n",
    "## Runtime approx 3m 15s ##\n",
    "## At this range the change in loss doesnt vary by more than 4, however k = 1500 still lowest so select that value for K ##\n",
    "## Could choose even tighter range, however time to run code is quite long ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Implement kernel k-means algorithm with RBF-kernel\n",
    "\n",
    "Use the first 500 samples and cluster the them into 5 groups. This is for reducing the running time of your code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1 and x2 are vectors\n",
    "def rbf_kernel(x1, x2, twosigmasquared):\n",
    "    # numerator  \n",
    "    numerator = -(np.square(abs(np.subtract(x1,x2))).sum())\n",
    "    # numerator = -np.linalg.norm(x1, x2)**2\n",
    "    denominator = twosigmasquared\n",
    "    result = np.exp(numerator/denominator)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbfHyperparamater(X):\n",
    "    n,d = X.shape\n",
    "    twosigmasquared = 0\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            twosigmasquared += np.square(abs(np.subtract(X[i],X[j]))).sum()\n",
    "    twosigmasquared = 1/n**2 * twosigmasquared\n",
    "    return twosigmasquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel_kmeans(X,k,twosigmasquared,if_plot):\n",
    "    n,d = X.shape\n",
    "    # Calculate kernel matrix (table) by precomputing kernel matrix for every point\n",
    "    Kernel = np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            Kernel[i][j] = rbf_kernel(X[i],X[j],twosigmasquared)\n",
    "\n",
    "    # Set inital cluster assignments, first 100 to k=1, 2nd 100 to k=2 ... last 100 to k=5\n",
    "    # r = np.array(random.randint(0, k - 1) for _ in range(n)])\n",
    "    r = np.zeros((n,k))\n",
    "    point = 0\n",
    "    for i in range(k):\n",
    "        for j in range(int(n/k)):\n",
    "            r[point][i] = 1\n",
    "            point = point + 1\n",
    "    \n",
    "    \n",
    "    print(\"intial clusters assignment matrix\", r)\n",
    "\n",
    "    # Set inital distances\n",
    "    distances_to_centers = np.zeros((n,k))\n",
    "\n",
    "    # While assignments changing\n",
    "    iterations = 0\n",
    "    while True:\n",
    "        r_old = r.copy()\n",
    "\n",
    "        # Calculate distances from each point to each cluster mean; i denotes cluster number\n",
    "        for i in range(k):\n",
    "            double_sum = 0\n",
    "            for m in range(n):\n",
    "                for l in range(n):\n",
    "                    # formula from lectures, r tells us which entry is assigned to a cluster\n",
    "                    double_sum += r[m][i] * r[l][i] * Kernel[m][l]\n",
    "            # m denotes sample number\n",
    "            for m in range(n):\n",
    "                term1 = rbf_kernel(X[m],X[m],twosigmasquared)\n",
    "                term2 = 2 / np.sum(r[:,i]) * np.sum(r[:,i] * Kernel[:,m])\n",
    "                # np.sum() gets number of assignments for ith cluster\n",
    "                term3 = 1 / np.sum(r[:,i])**2 * double_sum\n",
    "                distances_to_centers[m,i] =  term1 - term2 + term3\n",
    "        \n",
    "        # Find min distances and assign each point to their min (we go along columns to find cluster index)\n",
    "        closest_centers = np.argmin(distances_to_centers, axis=1)\n",
    "        r = np.zeros((n,k))\n",
    "        for i in range(n):\n",
    "            # our new assignment becomes the index of the closest cluster centroid\n",
    "            r[i][closest_centers[i]] = 1\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.all(r == r_old):\n",
    "            print(f\"convergence after {iterations} iterations\")\n",
    "            break\n",
    "        iterations = iterations + 1\n",
    "    return closest_centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features[:500]\n",
    "n,d = X.shape\n",
    "k = 10\n",
    "\n",
    "# Calculating twosigmasquared\n",
    "twosigmasquared = rbfHyperparamater(X)\n",
    "\n",
    "# Running RBF Kernel K Means\n",
    "labels_rbf_kmeans = rbf_kernel_kmeans(X,k,twosigmasquared,1)\n",
    "\n",
    "print(\"clustering\", labels_rbf_kmeans)\n",
    "\n",
    "### approx runtime: 31 seconds ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultant rbf clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = []\n",
    "images = [[]]*10\n",
    "for i in range(k):\n",
    "    clusters.append(X[labels_rbf_kmeans == i])\n",
    "    for j in range(10):\n",
    "        images[j] = clusters[i][j].reshape(28,28)\n",
    "    print(\"Images in cluster {}:\".format(i))\n",
    "    _, axs = plt.subplots(1, 10, figsize=(28, 28))\n",
    "    axs = axs.flatten()\n",
    "    for img, ax in zip(images, axs):\n",
    "        ax.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(data, dimensions):\n",
    "    centralized_data = data - np.mean(data , axis = 0)\n",
    "    cov_mat = np.cov(centralized_data , rowvar = False)\n",
    "    # for symmetric arrays\n",
    "    eigen_values , eigen_vectors = np.linalg.eigh(cov_mat)\n",
    "    # sort eigen values\n",
    "    top_k_eigenvalue_indexes = np.argsort(eigen_values)[::-1][:dimensions]\n",
    "    sorted_eigenvalue = eigen_values[top_k_eigenvalue_indexes]\n",
    "    # find the top k eigen values\n",
    "    sorted_eigenvectors = eigen_vectors[:,top_k_eigenvalue_indexes]\n",
    "    p = sorted_eigenvectors[:,:dimensions]\n",
    "\n",
    "    reduced_data = (p.T @ centralized_data.T).T\n",
    "    \n",
    "    return reduced_data, np.cov(reduced_data , rowvar = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(x, k = 10, max_iter = 50, threshold = 1e-3):\n",
    "        losses = []\n",
    "        prev_loss = 0\n",
    "        n = x.shape[0]\n",
    "        size = n // k\n",
    "        centroids = {}\n",
    "        # randomly assigning clusters\n",
    "        np.random.shuffle(x)\n",
    "        for i in range(k):\n",
    "            start = i * size\n",
    "            end = (i + 1) * size\n",
    "            centroids[i] = x[start:end,:]\n",
    "\n",
    "        for it in range(max_iter):\n",
    "            clusters = {}\n",
    "            for i in range(k):\n",
    "                clusters[i] = []\n",
    "\n",
    "            loss_sum = 0\n",
    "            for feature in x:\n",
    "                distances = [np.linalg.norm(feature - centroids[i]) for i in centroids]\n",
    "                loss_sum += np.sum(np.array(distances)**2)\n",
    "                cluster_index = distances.index(min(distances))\n",
    "                clusters[cluster_index].append(feature)\n",
    "            \n",
    "            # print(it, loss_sum)\n",
    "            losses.append(loss_sum)\n",
    "            prev_centroids = dict(centroids)\n",
    "\n",
    "            for cluster_index in clusters:\n",
    "                centroids[cluster_index] = np.average(clusters[cluster_index],axis=0)\n",
    "\n",
    "            if abs(loss_sum - prev_loss) < threshold:\n",
    "                print(f\"converge after {it} iterations\")\n",
    "                break\n",
    "            prev_loss = loss_sum\n",
    "\n",
    "        return clusters, losses\n",
    "        \n",
    "clusters, losses = k_means(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous rbf implementation that wasn't working\n",
    "\n",
    "def getHyperParameter(X):\n",
    "    n = X.shape[0] # 500\n",
    "    hyperparameter = 0\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            hyperparameter += np.linalg.norm(X[i]-X[j])**2\n",
    "    hyperparameter /= n**2\n",
    "    return hyperparameter\n",
    "\n",
    "\n",
    "\n",
    "def rbfKernel(X):\n",
    "    n = X.shape[0] \n",
    "    hyperparameter = getHyperParameter(X)\n",
    "    K=np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            K[i,j] = np.exp(-np.linalg.norm(X[i]-X[j])**2) / hyperparameter\n",
    "    return K\n",
    "\n",
    "def calculateDistances(Kernel, cluster_indexes, k = 10):\n",
    "    res = []\n",
    "    for cluster_num in range(k):\n",
    "        kernel_indexes = np.flatnonzero((cluster_num == cluster_indexes))\n",
    "        K = Kernel[kernel_indexes][:,kernel_indexes]\n",
    "        # print(K.shape)\n",
    "        n = K.shape[0]\n",
    "        distances = np.zeros((n,10))\n",
    "        k = 10\n",
    "        if n < 10:\n",
    "            k = n\n",
    "            distances.fill(sys.maxsize)\n",
    "        for j in range(k):\n",
    "            for i in range(n):\n",
    "                distances[i, j] = K[i, j] - 2 * sum(K[:,j]) / n + np.sum(K) / (n**2)\n",
    "        res.append(distances)\n",
    "\n",
    "    return np.array(res)\n",
    "\n",
    "def compute_loss(distances):\n",
    "    losses = 0\n",
    "    for row in distances:\n",
    "        for i in row:\n",
    "            if i == sys.maxsize: continue\n",
    "            losses += i**2\n",
    "    return losses\n",
    "\n",
    "def kernelKmean(X, iteration_limit = 30, threshold = 1e-4, k = 10):\n",
    "    K = rbfKernel(X)\n",
    "    n = K.shape[0]\n",
    "    losses = []\n",
    "    cluster_indexes = np.array([random.randint(0, k - 1) for _ in range(n)])\n",
    "    prev_loss = 0\n",
    "    # precomputing kernel\n",
    "    K = rbfKernel(X) \n",
    "    print(\"running k means\")\n",
    "    for it in range(iteration_limit):\n",
    "        loss_sum = 0\n",
    "        distances = calculateDistances(K, cluster_indexes)\n",
    "        distances = np.vstack(np.array(distances))\n",
    "    \n",
    "        loss_sum = compute_loss(distances)\n",
    "        losses.append(loss_sum)\n",
    "\n",
    "        cluster_indexes = np.array([np.argmin(i) for i in distances])\n",
    "        print(it, loss_sum)\n",
    "        if prev_loss == loss_sum:\n",
    "            print(f\"convergence achieved after {it} iterations\")\n",
    "            break\n",
    "        prev_loss = loss_sum\n",
    "       \n",
    "    return cluster_indexes, losses\n",
    "\n",
    "kernel_data = data[:500]\n",
    "clusters, losses = kernelKmean(kernel_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "336e0efb211540640d1eaf23f50c113d8410489ab7551eddc2cad419cd785df7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
