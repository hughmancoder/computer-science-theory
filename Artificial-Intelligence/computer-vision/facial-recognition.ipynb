{"cells":[{"cell_type":"markdown","metadata":{"id":"PyDkfc31TTAD"},"source":["# **Facial Expression Recognition Competition\n","**\n","For this competition, we will use the a facial classification(https://cloudstor.aarnet.edu.au/plus/s/8J44RsLu7uyRzhd) dataset. The data consists of 48x48 pixel grayscale images of faces. The faces have been automatically registered so that the face is more or less centred and occupies about the same amount of space in each image. You can download the CSV from this link (https://drive.google.com/file/d/1B_3ABybPrJKSkGJNSSJwctQijYOHcJZu/view)\n","\n","The task is to categorize each face based on the emotion shown in the facial expression into one of seven categories (0: Angry, 1: Disgust, 2: Fear, 3: Happy, 4: Sad, 5: Surprise, 6: Neutral). The training set consists of 28,709 examples and the public test set consists of 3,589 examples.\n","\n","We provide baseline code that includes the following features:\n","\n","*   Loding and Analysing the FER-2013 dataset using torchvision.\n","*   Defining a simple convolutional neural network.\n","*   How to use existing loss function for the model learning.\n","*   Train the network on the training data.\n","*   Test the trained network on the testing data.\n","*   Generate prediction for the random test image(s).\n","\n","The following changes could be considered:\n","-------\n","1. Change of advanced training parameters: Learning Rate, Optimizer, Batch-size, Number of Max Epochs, and Drop-out.\n","2. Use of a new loss function.\n","3. Data augmentation\n","4. Architectural Changes: Batch Normalization, Residual layers, Attention Block, and other varients.\n","\n","Marking Rules:\n","-------\n","We will mark the competition based on the final test accuracy on testing images and your report.\n","\n","Final mark (out of 50) = acc_mark + efficiency mark + report mark\n","###Acc_mark 10:\n","\n","We will rank all the submission results based on their test accuracy. Zero improvement over the baseline yields 0 marks. Maximum improvement over the baseline will yield 10 marks. There will be a sliding scale applied in between.\n","\n","###Efficiency mark 10:\n","\n","Efficiency considers not only the accuracy, but the computational cost of running the model (flops: https://en.wikipedia.org/wiki/FLOPS). Efficiency for our purposes is defined to be the ratio of accuracy (in %) to Gflops. Please report the computational cost for your final model and include the efficiency calculation in your report. Maximum improvement over the baseline will yield 10 marks. Zero improvement over the baseline yields zero marks, with a sliding scale in between.\n","\n","###Report mark 30:\n","Your report should comprise:\n","1. An introduction showing your understanding of the task and of the baseline model: [10 marks]\n","\n","2. A description of how you have modified aspects of the system to improve performance. [10 marks]\n","\n","A recommended way to present a summary of this is via an \"ablation study\" table, eg:\n","\n","|Method1|Method2|Method3|Accuracy|\n","|---|---|---|---|\n","|N|N|N|60%|\n","|Y|N|N|65%|\n","|Y|Y|N|77%|\n","|Y|Y|Y|82%|\n","\n","3. Explanation of the methods for reducing the computational cost and/or improve the trade-off between accuracy and cost: [5 marks]\n","\n","4. Limitations/Conclusions: [5 marks]\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4204,"status":"ok","timestamp":1687400687092,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"Kx1KHLh6TOFY"},"outputs":[],"source":["# Importing libraries.\n","\n","import torch\n","import torchvision\n","import tarfile\n","import torch.nn as nn\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# To avoid non-essential warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","%matplotlib inline\n","from tqdm import tqdm\n","import torchvision.transforms as T\n","from torchvision.datasets import ImageFolder\n","from torchvision.transforms import ToTensor\n","from torchvision.utils import make_grid\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21036,"status":"ok","timestamp":1687400708121,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"da5uVjJKThYk","outputId":"de56000c-0739-4bd4-9f48-b53a246df5f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# Mounting G-Drive to get your dataset.\n","# To access Google Colab GPU; Go To: Edit >>> Network Settings >>> Hardware Accelarator: Select GPU.\n","# Reference: https://towardsdatascience.com/google-colab-import-and-export-datasets-eccf801e2971\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":210},"executionInfo":{"elapsed":302,"status":"error","timestamp":1687400732674,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"nswXsMf2j_ql","outputId":"4b339214-9cbe-4896-9f19-2d9fb1caf0c1"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-838d2f1d6e68>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata_directory\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Collab Notebooks/cv-assignment-4/fer2013.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Dataset path exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/cv-assignment-4/fer2013.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"]}],"source":["improt os\n","# # Dataset path.\n","# adrians_dir ='/content/drive//MyDrive/Colab Notebooks/cv-assignment-4/fer2013.csv'\n","# data_directory ='/content/drive/MyDrive/Collab Notebooks/cv-assignment-4/fer2013.csv'\n","# # Dataset path exists\n","# os.path.exists('/content/drive/MyDrive/Colab Notebooks/cv-assignment-4/fer2013.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":25,"status":"aborted","timestamp":1687400709544,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"Q1842ueQTxnp"},"outputs":[],"source":["# Reading the dataset file using Pandas read_csv function and print the first\n","# 5 samples.\n","#\n","# Reference: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\n","# data_df = pd.read_csv(adrians_dir)\n","data_df = pd.read_csv(data_directory)\n","data_df.head(4)\n","\n","# Mapping of the Facial Expression Labels.\n","Labels = {\n","    0:'Angry',\n","    1:'Disgust',\n","    2:'Fear',\n","    3:'Happy',\n","    4:'Sad',\n","    5:'Surprise',\n","    6:'Neutral'\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":25,"status":"aborted","timestamp":1687400709544,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"L-7qJKDkT9Yr"},"outputs":[],"source":["# Categorizing the dataset to three categories.\n","# Training: To train the model.\n","# PrivateTest: To test the train model; commonly known as Validation.\n","# PublicTest: To test the final model on Test set to check how your model perfomed. Do not use this data as your validation data.\n","train_df = data_df[data_df['Usage']=='Training']\n","valid_df = data_df[data_df['Usage']=='PublicTest']\n","test_df = data_df[data_df['Usage']=='PrivateTest']\n","print(train_df.head())\n","print(valid_df.head(-1))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24,"status":"aborted","timestamp":1687400709544,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"8QjeGOPqT_Ts"},"outputs":[],"source":["# Test-check to see wether usage labels have been allocated to the dataset/not.\n","valid_df = valid_df.reset_index(drop=True)\n","test_df = test_df.reset_index(drop=True)\n","print(test_df.head())\n","print('   -----   -------    -------    --------     -----    -------')\n","print(valid_df.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24,"status":"aborted","timestamp":1687400709544,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"r-xYa-jlUBIi"},"outputs":[],"source":["# Preview of the training sample and associated labels.\n","def show_example(df, num):\n","    expression_index = int(df.loc[num, ['emotion']])\n","    print(expression_index)\n","\n","    print('expression: ', Labels[expression_index])\n","    image = np.array([[int(i) for i in x.split()] for x in df.loc[num, ['pixels']]])\n","    image = image.reshape(48,48)\n","    plt.imshow(image, interpolation='nearest', cmap='gray')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24,"status":"aborted","timestamp":1687400709544,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"AZPA0K3XYaql"},"outputs":[],"source":["show_example(train_df, 107)\n","show_example(train_df, 343)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":25,"status":"aborted","timestamp":1687400709545,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"ShizdbgsUdTI"},"outputs":[],"source":["import random\n","import PIL\n","\n","# Normalization of the train and validation data.\n","class expressions(Dataset):\n","    def __init__(self, df, transforms=None, augment_transforms=None, augment_size = 0.2):\n","        self.df = df.copy()\n","        self.df['pixels'] = self.df['pixels'].apply(lambda x: np.array(x.split(), dtype=np.uint8))\n","\n","        self.augment_transforms = augment_transforms\n","        self.transforms = transforms\n","\n","        # augment the dataset if specified\n","        if augment_transforms is not None:\n","            num_rows = len(self.df)\n","            num_rows_to_loop = int(augment_size * num_rows)  # % of the rows based on augment size\n","            row_indices = random.sample(range(num_rows), num_rows_to_loop)\n","            print('shape before augmentation:', self.df['pixels'].shape)\n","            augmented_data = []\n","            for index in row_indices:\n","                row = self.df.loc[index]\n","                image = np.array(row['pixels'], dtype=np.uint8).reshape(48, 48, 1)\n","                augmented_image = augment_transforms(PIL.Image.fromarray(image.squeeze(), mode='L'))\n","                augmented_image_np = np.array(augmented_image)\n","                augmented_row = {\n","                    'emotion': row['emotion'],\n","                    'pixels': augmented_image_np,\n","                    'Usage': row['Usage']\n","                }\n","                augmented_data.append(augmented_row)\n","            augmented_df = pd.DataFrame(augmented_data)\n","            # Concatenate the original and augmented_df\n","            self.df = pd.concat([self.df, augmented_df], ignore_index=True)\n","            print('shape after augmentation:', self.df['pixels'].shape)\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, index):\n","        row = self.df.loc[index]\n","        image = np.array(row['pixels'])\n","        label = row['emotion']\n","        image = np.asarray(image).reshape(48, 48, 1)\n","\n","        # transform image as it is retreived\n","        if self.transforms:\n","            image = self.transforms(image)\n","\n","        return image, label\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":25,"status":"aborted","timestamp":1687400709545,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"8_bY6dp9UgIA"},"outputs":[],"source":["#import albumentations as A\n","stats = ([0.5],[0.5])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":25,"status":"aborted","timestamp":1687400709545,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"tY-VDCynUicG"},"outputs":[],"source":["train_tsfm = T.Compose([\n","    T.ToPILImage(),\n","    T.Grayscale(num_output_channels=1),\n","    T.ToTensor(),\n","    T.Normalize(*stats,inplace=True),\n","])\n","valid_tsfm = T.Compose([\n","    T.ToPILImage(),\n","    T.Grayscale(num_output_channels=1),\n","    T.ToTensor(),\n","    T.Normalize(*stats,inplace=True)\n","])\n","\n","# NOTE: augmentations defined here\n","augment_tsfm = T.Compose([\n","    T.RandomHorizontalFlip(),\n","    T.RandomRotation(10),\n","    T.RandomAffine(degrees=0, translate=(0.1, 0.1)), #  affine transormation includes rotation, translation, scaling, and shearing operations; degrees = 0 does not apply a rotation\n","    # T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1) # randomly adjusts the brightness, contrast, saturation, and hue of the image.\n","])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24,"status":"aborted","timestamp":1687400709545,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"b1POsgNqUkM0"},"outputs":[],"source":["valid_ds = expressions(valid_df, valid_tsfm)\n","test_ds = expressions(test_df, valid_tsfm)\n","train_ds = expressions(train_df, train_tsfm)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24,"status":"aborted","timestamp":1687400709545,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"k82Tgkm1ulK5"},"outputs":[],"source":["import math\n","def show_df_pixels(df, sample_size=10):\n","    images = df.sample(sample_size)['pixels'].values\n","    num_images = len(images)\n","    num_cols = 10\n","    num_rows = math.ceil(num_images / num_cols)\n","\n","    fig, axs = plt.subplots(num_rows, num_cols, figsize=(12, 6))\n","    fig.subplots_adjust(hspace=0, wspace=0)\n","\n","    for i, ax in enumerate(axs.flat):\n","        ax.imshow(np.array(images[i]).reshape(48, 48), cmap='gray')\n","        ax.axis('off')\n","        if i >= num_images - 1:\n","            break\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24,"status":"aborted","timestamp":1687400709545,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"gVyHrt_5zIW5"},"outputs":[],"source":["\n","print('train set size', train_ds.df.shape)\n","print('augmented validation set size', valid_ds.df.shape)\n","\n","print('non augmented df')\n","show_df_pixels(train_ds.df.tail(10),10)\n","print('augmented df')\n","show_df_pixels(train_ds_augmented.df.tail(10), 10)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24,"status":"aborted","timestamp":1687400709545,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"g2zV2cuDUxFl"},"outputs":[],"source":["# Evaluation metric - Accuracy in this case.\n","import torch.nn.functional as F\n","input_size = 48*48\n","output_size = len(Labels)\n","\n","def accuracy(output, labels):\n","    predictions, preds = torch.max(output, dim=1)\n","    return torch.tensor(torch.sum(preds==labels).item()/len(preds))"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":322,"status":"ok","timestamp":1687400755533,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"Qvl3olxrU2W5"},"outputs":[],"source":["# Expression model class for training and validation purpose.\n","\n","class expression_model(nn.Module):\n","    def training_step(self, batch):\n","        images, labels = batch\n","        out = self(images)\n","        loss = F.cross_entropy(out, labels)\n","        return loss\n","\n","    def validation_step(self, batch):\n","        images, labels = batch\n","        out = self(images)\n","        loss = F.cross_entropy(out, labels)\n","        acc = accuracy(out, labels)\n","        return {'val_loss': loss.detach(), 'val_acc': acc}\n","\n","    def validation_epoch_end(self, outputs):\n","        batch_losses = [x['val_loss'] for x in outputs]\n","        epoch_loss = torch.stack(batch_losses).mean()\n","        batch_acc = [x['val_acc'] for x in outputs]\n","        epoch_acc = torch.stack(batch_acc).mean()\n","        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n","\n","    def epoch_end(self, epoch, result):\n","        print(\"Epoch[{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1687400757941,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"9dWl9SByU-wR","outputId":"dd5e2363-747c-43bf-f348-09a8938fa2b1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on device cuda\n"]}],"source":["def get_default_device():\n","    \"\"\"Pick GPU if available, else CPU\"\"\"\n","    if torch.cuda.is_available():\n","        return torch.device('cuda')\n","    else:\n","        return torch.device('cpu')\n","\n","device = get_default_device()\n","print(f'Running on device {device}') # cuda = gpu"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":293,"status":"ok","timestamp":1687400760285,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"XBiFaKAKVEo0"},"outputs":[],"source":["def to_device(data, device):\n","    \"\"\"Move tensor(s) to chosen device\"\"\"\n","    if isinstance(data, (list,tuple)):\n","        return [to_device(x, device) for x in data]\n","    return data.to(device, non_blocking=True)\n","\n","class DeviceDataLoader():\n","    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n","    def __init__(self, dl, device):\n","        self.dl = dl\n","        self.device = device\n","\n","    def __iter__(self):\n","        \"\"\"Yield a batch of data after moving it to device\"\"\"\n","        for b in self.dl:\n","            yield to_device(b, self.device)\n","\n","    def __len__(self):\n","        \"\"\"Number of batches\"\"\"\n","        return len(self.dl)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24,"status":"aborted","timestamp":1687400709546,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"_WYYWRLKe1QF"},"outputs":[],"source":["# Basic model - 1 layer\n","simple_model = nn.Sequential(\n","    nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1),\n","    nn.MaxPool2d(2, 2)\n",")"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":247},"executionInfo":{"elapsed":6,"status":"error","timestamp":1687400762735,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"XDIiH4mXf1Hc","outputId":"289066df-f143-4525-d73c-9a5aad54a14d"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-765fc5b7d19e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain_dataset_augmented\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds_augmented\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvalid_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_ds' is not defined"]}],"source":["batch_size = 400\n","train_dataset = DataLoader(train_ds, batch_size, shuffle=True, num_workers=2)\n","train_dataset_augmented = DataLoader(train_ds_augmented, batch_size, shuffle=True, num_workers=2)\n","valid_dataset = DataLoader(valid_ds, batch_size, num_workers=2)\n","test_dataset = DataLoader(test_ds, batch_size, num_workers=2)\n","\n","train_dl = DeviceDataLoader(train_dataset, device)\n","valid_dl = DeviceDataLoader(valid_dataset, device)\n","test_dl = DeviceDataLoader(test_dataset, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24,"status":"aborted","timestamp":1687400709546,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"IJouDbTIe3vg"},"outputs":[],"source":["for images, labels in train_dl:\n","    print('images.shape:', images.shape)\n","    out = simple_model.to(device)(images)\n","    print('out.shape:', out.shape)\n","    break"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":450,"status":"ok","timestamp":1687400766326,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"B5dmt4X6VO-l"},"outputs":[],"source":["# Model - 7 layer\n","class expression(expression_model):\n","    def __init__(self,classes=7):\n","        super().__init__()\n","        self.num_classes = classes\n","        self.network = nn.Sequential(\n","            nn.Conv2d(1, 8, kernel_size=3, padding=1),  #(input channels, output channels)\n","            nn.ReLU(),\n","            nn.Conv2d(8, 32, kernel_size=3, padding=1),  #(input channels, output channels)\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2), # output: 64 x 24 x 24\n","\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2), # output: 128 x 12 x 12\n","\n","            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2), # output: 256 x 6 x 6\n","\n","            nn.Flatten(),\n","            nn.Linear(256*6*6, 2304),\n","            nn.ReLU(),\n","            nn.Linear(2304, 1152),\n","            nn.ReLU(),\n","            nn.Linear(1152, 576),\n","            nn.ReLU(),\n","            nn.Linear(576,288),\n","            nn.ReLU(),\n","            nn.Linear(288,144),\n","            nn.ReLU(),\n","            nn.Linear(144,self.num_classes))\n","            # 144 to 6\n","\n","\n","    def forward(self, xb):\n","        return self.network(xb)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":786,"status":"ok","timestamp":1687400860422,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"qXd0ipgo6stm","outputId":"8c4144e5-148b-4029-fa00-2936ff009a62"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total trainable parameters: 25891863\n","Total parameters: 25891863\n"]}],"source":["model = expression()\n","total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(f\"Total trainable parameters: {total_params}\")\n","\n","total_params = sum(p.numel() for p in model.parameters())\n","print(f\"Total parameters: {total_params}\")"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":394},"executionInfo":{"elapsed":5,"status":"error","timestamp":1687400855010,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"lAThCMMfcfEg","outputId":"83ba0fd8-6070-4db7-e1e7-9caa77b6f76e"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-59902728bf70>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# !mv download FLOPs_counter.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# !rm -rf download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mFLOPs_counter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_model_parm_flops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# default_input_size = torch.randn(1, 1, 48, 48) # model input size (N, C, H, W)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcomputeFlops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'FLOPs_counter'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}],"source":["# !wget -c https://cloudstor.aarnet.edu.au/plus/s/hXo1dK9SZqiEVn9/download\n","# !mv download FLOPs_counter.py\n","# !rm -rf download\n","from FLOPs_counter import print_model_parm_flops\n","# default_input_size = torch.randn(1, 1, 48, 48) # model input size (N, C, H, W)\n","def computeFlops(model, input):\n","  print_model_parm_flops(model, input, detail=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":25,"status":"aborted","timestamp":1687400709547,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"gVPwNTi5egtz"},"outputs":[],"source":["# Plots for accuracy and loss during training period.\n","def plot_accuracies(history):\n","    accuracies = [x['val_acc'] for x in history]\n","    plt.plot(accuracies, '-x')\n","    plt.xlabel('epoch')\n","    plt.ylabel('accuracy')\n","    plt.title('Accuracy vs. No. of epochs')\n","    plt.show()\n","\n","def plot_losses(history):\n","    train_losses = [x.get('train_loss') for x in history]\n","    val_losses = [x['val_loss'] for x in history]\n","    plt.plot(train_losses, '-bx')\n","    plt.plot(val_losses, '-rx')\n","    plt.xlabel('epoch')\n","    plt.ylabel('loss')\n","    plt.legend(['Training', 'Validation'])\n","    plt.title('Loss vs. No. of epochs')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":25,"status":"aborted","timestamp":1687400709547,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"8ms49hovVTSI"},"outputs":[],"source":["\"\"\"This function is responsible for evaluating the model's performance on a validation dataset\"\"\"\n","@torch.no_grad()\n","def evaluate(model, valid_dl):\n","    model.eval() # disable any regularization techniques that might affect the evaluation\n","    outputs = [model.validation_step(batch) for batch in valid_dl]\n","    return model.validation_epoch_end(outputs)\n","\n","def train(epochs, lr, model, train_dl, valid_dl, opt_func=torch.optim.SGD):\n","    history = []\n","    optimizer = opt_func(model.parameters(), lr)\n","    for epoch in range(epochs):\n","        # Training Phase\n","        model.train()\n","        train_losses = []\n","        for batch in train_dl:\n","            loss = model.training_step(batch)\n","            train_losses.append(loss)\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","        # Validation phase\n","        result = evaluate(model, valid_dl)\n","        print(f'epoch {epoch} result {result}')\n","        result['train_loss'] = torch.stack(train_losses).mean().item()\n","        model.epoch_end(epoch, result)\n","        history.append(result)\n","    return history"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24,"status":"aborted","timestamp":1687400709547,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"RX0S_Li5v13o"},"outputs":[],"source":["from torch.cuda.amp import GradScaler, autocast\n","from torch.optim.lr_scheduler import StepLR\n","\"\"\" improved training function with optional parameters \"\"\"\n","# train_gpu(num_epochs, lr, model_3, train_dl, valid_dl, opt_fn, stop_on_convergence=False, lr_scheduler=True, display_running_loss=True, convergence_threshold=0.001)\n","def train_gpu(epochs, lr, model, train_dl, valid_dl, opt_func, stop_on_convergence=True, convergence_threshold=0.001, display_running_loss=True):\n","    history = []\n","    optimizer = opt_func(model.parameters(), 0.001) if lr is None else opt_func(model.parameters(), lr)\n","    last_loss = None\n","    scaler = GradScaler()\n","    for epoch in range(epochs):\n","        model.train()\n","        running_loss = 0\n","        train_losses = []\n","        epoch_quarter = len(train_dl) // 4\n","        if lr is None:\n","          scheduler = StepLR(optimizer, step_size=7, gamma=0.1) # LR scheduler\n","\n","        for i, batch in enumerate(train_dl):\n","            # Move batch to GPU\n","            inputs, targets = batch\n","            inputs = inputs.to('cuda')\n","            targets = targets.to('cuda')\n","\n","            # Forward and backward pass with mixed precision\n","            optimizer.zero_grad()\n","            with torch.cuda.amp.autocast():\n","                loss = model.training_step((inputs, targets))\n","                scaler.scale(loss).backward()\n","\n","            scaler.step(optimizer)\n","            scaler.update()\n","\n","            running_loss += loss.item()\n","            train_losses.append(loss.detach())\n","\n","            if display_running_loss and (i+1) % epoch_quarter == 0:  # print culmulative sum of losses every quarter epoch\n","              print(f'Running loss: {running_loss}')\n","              running_loss = 0\n","\n","        # learning rate scheduling\n","        if lr is None:\n","          scheduler.step()\n","\n","        # Validation\n","        result = evaluate(model, valid_dl)\n","        result['train_loss'] = torch.stack(train_losses).mean().item()\n","        model.epoch_end(epoch, result)\n","        history.append(result)\n","\n","         # Stop condition\n","        if stop_on_convergence and last_loss is not None and abs(last_loss - result['val_loss']) < convergence_threshold:\n","            print(f'stopping training as convergence threshold reached after {epoch} epochs with loss {last_loss}')\n","            break\n","        last_loss = result['val_loss']\n","    return history"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24,"status":"aborted","timestamp":1687400709547,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"O1YTDcbsgRIc"},"outputs":[],"source":["def getDataLoaders(train_ds, valid_ds, test_ds, batch_size):\n","  train_dataset = DataLoader(train_ds, batch_size, shuffle=True, num_workers=2)\n","  valid_dataset = DataLoader(valid_ds, batch_size, num_workers=2)\n","  test_dataset = DataLoader(test_ds, batch_size, num_workers=2)\n","\n","  train_dl = DeviceDataLoader(train_dataset, device)\n","  valid_dl = DeviceDataLoader(valid_dataset, device)\n","  test_dl = DeviceDataLoader(test_dataset, device)\n","  return train_dl, valid_dl, test_dl"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24,"status":"aborted","timestamp":1687400709547,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"vKgfIaC8Bvmc"},"outputs":[],"source":["def evaluateModel(model_,history_, test_dl, batch_size):\n","  res = evaluate(model_, test_dl) # final result\n","  print(f\"test accuracy: {res['val_acc']}\")\n","  input = torch.randn(batch_size, 1, 48, 48)\n","  input = input.to(device)\n","  computeFlops(model_, input)\n","  plot_accuracies(history_)\n","  plot_losses(history_)"]},{"cell_type":"markdown","metadata":{"id":"geasw86TbmOz"},"source":["### Changes to consider\n","- [x] Learning Rate\n","- [x] Optimizer (Nadam is used but could trial other ones)\n","- [x] Batch-size\n","- [x] Number of Max Epochs\n","- [x] Dropout\n","- [ ] Use of a new loss function (requires creating new evaluate function for new loss function)\n","- [x] Data augmentation\n","- [x] Architectural Changes:\n","  - [x] Batch Normalization\n","  - [x] Residual layers\n","- [ ] Use Pretrained Models: Transfer learning"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24,"status":"aborted","timestamp":1687400709547,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"kxK5NM6fkwAQ"},"outputs":[],"source":["# Train 1: find best learning rates\n","opt_fn=torch.optim.Adam\n","batch_size = 256\n","train_dl, valid_dl, test_dl = getDataLoaders(train_ds, valid_ds, test_ds, batch_size)\n","num_epochs = 15\n","\n","learning_rates = [None, 0.001, 0.01]\n","train_0_histories = []\n","\n","for idx, lr in enumerate(learning_rates):\n","    model_name = f'model_{idx}'\n","    print(f'training {model_name} with lr {lr}')\n","    model = to_device(expression(classes = 7), device)\n","    model_history = train_gpu(num_epochs, lr, model, train_dl, valid_dl, opt_fn, stop_on_convergence=True)\n","    train_0_histories.append([model.to(device), model_history])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24,"status":"aborted","timestamp":1687400709547,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"Y2hluKO4yLkG"},"outputs":[],"source":["for i, lr in enumerate(learning_rates):\n","    print(f'evaluating model {i} with lr {lr}')\n","    model  = train_0_histories[i][0]\n","    history = train_0_histories[i][1]\n","    evaluateModel(model, history, test_dl, batch_size)"]},{"cell_type":"markdown","metadata":{"id":"XG6RmO87wLnQ"},"source":["### Model 0: tuning Learning Rate\n","Learning rate scheduling scored the highest accuracy at 56%\n","\n","efficiency = accuracy % / Gflops\n","# Learning rate summary\n","|model|lr|epochs needed to converge|test_accuracy|flops|efficiency|\n","|--------|---------|--|------|-------|----|\n","| model0 |scheduler|14|0.5636|13.28G |4.25|\n","| model1 |0.001    |8 |0.5443|13.28G |4.09|\n","| model2 |0.01     |7 |0.2550|13.28G |1.92|\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":25,"status":"aborted","timestamp":1687400709548,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"c04_hESq1Ss-"},"outputs":[],"source":["# Train 2: find best batch size\n","opt_fn=torch.optim.Adam\n","batch_size = 256\n","num_epochs = 15\n","\n","learning_rate = None # lr scheduling\n","batch_sizes = [64,128,256]\n","train_1_histories = []\n","\n","for idx, batch_size in enumerate(batch_sizes):\n","    model_name = f'model_{idx}'\n","    print(f'training {model_name} with batch size {batch_size}')\n","    train_dl, valid_dl, test_dl = getDataLoaders(train_ds, valid_ds, test_ds, batch_size)\n","    model = to_device(expression(classes = 7), device)\n","    model_history = train_gpu(num_epochs,learning_rate, model, train_dl, valid_dl, opt_fn, stop_on_convergence=True)\n","    train_1_histories.append([model.to(device), model_history])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Oq6IjO1_tr9"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":25,"status":"aborted","timestamp":1687400709548,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"OuRuAYf_2kPS"},"outputs":[],"source":["for i, bs in enumerate(batch_sizes):\n","    print(f'evaluating model {i} with batch size {bs}')\n","    model  = train_1_histories[i][0]\n","    history = train_1_histories[i][1]\n","    evaluateModel(model, history, test_dl, bs)"]},{"cell_type":"markdown","metadata":{"id":"tNBZkYBY5IcL"},"source":["### Train 2: tuning batch size\n","A batch size of 128 seems  strikes a good balance in terms of accuracy\n","\n","|model|batch size|epochs|test_accuracy|flops|efficiency|\n","|--------|---|--|------|-------|-----|\n","| model0 |64 |15|0.5592|3.77G  |14.83|\n","| model1 |128|15|0.5908|6.94G  |08.50|\n","| model2 |256|15|0.5530|13.28G |04.16|\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":25,"status":"aborted","timestamp":1687400709548,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"umGC7-9V9UdB"},"outputs":[],"source":["# Train 3: choosing best loss function\n","num_epochs = 15\n","learning_rate = None\n","loss_functions = [torch.optim.Adam, torch.optim.SGD, torch.optim.RMSprop]\n","train_3_histories = []\n","\n","for idx, loss_fn in enumerate(loss_functions):\n","    model_name = f'model_{idx}'\n","    print(f'training {model_name} with loss function {loss_fn}')\n","    train_dl, valid_dl, test_dl = getDataLoaders(train_ds, valid_ds, test_ds, batch_size)\n","    model = to_device(expression(classes = 7), device)\n","    model_history = train_gpu(num_epochs,learning_rate, model, train_dl, valid_dl, opt_fn, stop_on_convergence=True)\n","    train_3_histories.append([model.to(device), model_history])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":25,"status":"aborted","timestamp":1687400709548,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"uUYiLE2W-QRf"},"outputs":[],"source":["for i, loss_fn in enumerate(loss_functions):\n","    print(f'evaluating model {i} with loss function {loss_fn}')\n","    model  = train_3_histories[i][0]\n","    history = train_3_histories[i][1]\n","    evaluateModel(model, history, test_dl, batch_size)"]},{"cell_type":"markdown","metadata":{"id":"DhL27_nvADJ4"},"source":["### Train 3: choosing best optimizer\n","Both SGD and adam provide best optimization with similar performance, however Adam appears to be more stable as it has less fluctuations and will be used\n","\n","|model|optimizer|epochs|test_accuracy|flops|efficiency|\n","|--------|---|--|------|-------|-----|\n","| model0 |Adam   |15|0.5700|6.94G  |8.22|\n","| model1 |SGD    |15|0.5751|6.94G  |8.29|\n","| model2 |RMSprop|15|0.5488|6.94G |7.92|\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yAJe1ECl9NTh"},"source":["# changing neural network architecture\n","model has been altered to include Batch Normalization after each convolution and Dropout after each max pooling. The dropout rate of 0.5 means that approximately half of the neurons in the layer will be \"turned off\" and outputs are set to 0during each forward pass. This should help prevent overfitting and improve generalisation"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":313,"status":"ok","timestamp":1687400947076,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"Hk7PjWd19QXB"},"outputs":[],"source":["class expressionsModified(expression_model):\n","    def __init__(self, classes=7):\n","        super().__init__()\n","        self.num_classes = classes\n","        self.network = nn.Sequential(\n","            nn.Conv2d(1, 8, kernel_size=3, padding=1), #(input channels, output channels)\n","            nn.BatchNorm2d(8),\n","            nn.ReLU(),\n","            nn.Conv2d(8, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2), # output: 64 x 24 x 24\n","            nn.Dropout(0.25),\n","\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2), # output: 128 x 12 x 12\n","            nn.Dropout(0.25),\n","\n","            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2), # output: 256 x 6 x 6\n","            nn.Dropout(0.25),\n","\n","            nn.Flatten(),\n","            nn.Linear(256*6*6, 2304),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(2304, 1152),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(1152, 576),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(576,288),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(288,144),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(144,self.num_classes)\n","        )\n","\n","    def forward(self, xb):\n","        return self.network(xb)"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":886,"status":"ok","timestamp":1687400959342,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"BBg2USdg7yXj","outputId":"fb114990-1cad-4ab3-be8c-0e824068d6c8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total trainable parameters: 25893607\n","Total parameters: 25893607\n"]}],"source":["model = expressionsModified()\n","total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(f\"Total trainable parameters: {total_params}\")\n","\n","total_params = sum(p.numel() for p in model.parameters())\n","print(f\"Total parameters: {total_params}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":25,"status":"aborted","timestamp":1687400709548,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"tCGVWz9OOdvo"},"outputs":[],"source":["# Train 4: training on modified arhictecture with dropout and batch normalization applied\n","num_epochs = 40\n","batch_size = 128\n","learning_rate = None\n","loss_fn = torch.optim.Adam\n","train_dl, valid_dl, test_dl = getDataLoaders(train_ds, valid_ds, test_ds, batch_size)\n","model_4 = to_device(expressionsModified(classes = 7), device)\n","model_4_history = train_gpu(num_epochs,learning_rate, model_4, train_dl, valid_dl, opt_fn, stop_on_convergence=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":25,"status":"aborted","timestamp":1687400709548,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"eAVio6FhJNvy"},"outputs":[],"source":["evaluateModel(model_4, model_4_history, test_dl, batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":26,"status":"aborted","timestamp":1687400709549,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"6QzoXxrKMj1i"},"outputs":[],"source":["# Train 5: training best performing model using data augmentation with of size 150% of the training dataset\n","train_ds_augmented = expressions(train_df, train_tsfm, augment_tsfm, augment_size = 0.5)\n","num_epochs = 60\n","batch_size = 128\n","learning_rate = None\n","loss_fn = torch.optim.Adam\n","train_dl, valid_dl, test_dl = getDataLoaders(train_ds_augmented, valid_ds, test_ds, batch_size)\n","model_5 = to_device(expressions(classes = 7), device)\n","model_5_history = train_gpu(num_epochs,learning_rate, model_5, train_dl, valid_dl, opt_fn, stop_on_convergence=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":25,"status":"aborted","timestamp":1687400709549,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"-zbLRHp2SESA"},"outputs":[],"source":["evaluateModel(model_5, model_5_history, test_dl, batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":25,"status":"aborted","timestamp":1687400709549,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"lqAcyp5GSto0"},"outputs":[],"source":["# Train 6: training best performing model using data augmentation with of size 150% of the training dataset\n","num_epochs = 20\n","batch_size = 128\n","learning_rate = None\n","loss_fn = torch.optim.Adam\n","train_dl, valid_dl, test_dl = getDataLoaders(train_ds_augmented, valid_ds, test_ds, batch_size)\n","model_6 = to_device(expressionsModified(classes = 7), device)\n","model_6_history = train_gpu(num_epochs,learning_rate, model_6, train_dl, valid_dl, opt_fn, stop_on_convergence=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":25,"status":"aborted","timestamp":1687400709549,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"reyySDgvSuBj"},"outputs":[],"source":["evaluateModel(model_6, model_6_history, test_dl, batch_size)"]},{"cell_type":"markdown","metadata":{"id":"AMUhKZTxVlEJ"},"source":["Best_model = model_5 (model with the highest accuracy)\n","\n","| Model   | Architecture | Epochs | Train Accuracy | FLOPs | Efficiency | Augmented Dataset |\n","|---------|--------------|--------|----------------|-------|------------|-------------------|\n","| model4  | Modified     | 40     | 0.3858         | 6.94G | 5.56       | N                 |\n","| model5  | Baseline     | 17     | 0.6057         | 6.94G | 8.73       | Y                 |\n","| model6  | Modified     | 25     | 0.5554         | 6.94G | 8.00       | Y                 |\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":25,"status":"aborted","timestamp":1687400709549,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"28zn6GWwVwrQ"},"outputs":[],"source":["# Prediction function to evaluate the model.\n","def predict_image(img, model):\n","    xb = img.unsqueeze(0)\n","    yb = model(xb)\n","    _, preds  = torch.max(yb, dim=1)\n","    return Labels[preds[0].item()]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":25,"status":"aborted","timestamp":1687400709549,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"XIFbEoS-V3MP"},"outputs":[],"source":["\n","best_model = model_5\n","img, label = test_ds[0]\n","plt.imshow(img[0], interpolation='nearest', cmap='gray')\n","img = img.to(device)\n","print('Label:', Labels[label], ', Predicted:', predict_image(img, best_model))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":25,"status":"aborted","timestamp":1687400709549,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"i88tAsWWV2ej"},"outputs":[],"source":["# making 10 predictions\n","img_index = 110\n","for i in range(10):\n","  img, label = test_ds[img_index + i]\n","  plt.imshow(img[0], interpolation='nearest', cmap='gray')\n","  plt.show()  # Display the image\n","  img = img.to(device)\n","  print('Label:', Labels[label], ', Predicted:', predict_image(img, best_model), '\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":26,"status":"aborted","timestamp":1687400709550,"user":{"displayName":"Hugh Signoriello","userId":"05908382042323199435"},"user_tz":-570},"id":"6XzHrJOUgag9"},"outputs":[],"source":["!sudo apt-get install texlive-xetex texlive-fonts-recommended\n","!jupyter nbconvert --to pdf --no-input --TagRemovePreprocessor.remove_cell_tags 'remove-cell' '/content/drive/MyDrive/Collab Notebooks/cv-assignment-4/Assignment4.ipynb'\n","!jupyter nbconvert  --to pdf '/content/drive/MyDrive/Collab Notebooks/cv-assignment-3/Assignment3.ipynb'"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
